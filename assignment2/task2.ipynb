{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "74270e2e-2bcd-49d3-85f3-450aa7a73d34",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f442a245",
    "execution_start": 1645035519226,
    "execution_millis": 1850,
    "deepnote_cell_type": "code"
   },
   "source": "%run task2a.ipynb\nimport numpy as np\nimport utils\nimport matplotlib.pyplot as plt\n#from task2a import cross_entropy_loss, SoftmaxModel, one_hot_encode, pre_process_images\nfrom trainer import BaseTrainer\nnp.random.seed(0)",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6d9d448f-271c-40d0-b8e6-0679c96dc6e5",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "55ab6978",
    "execution_start": 1645035521081,
    "execution_millis": 16,
    "deepnote_cell_type": "code"
   },
   "source": "def calculate_accuracy(X: np.ndarray, targets: np.ndarray, model: SoftmaxModel) -> float:\n    \"\"\"\n    Args:\n        X: images of shape [batch size, 785]\n        targets: labels/targets of each image of shape: [batch size, 10]\n        model: model of class SoftmaxModel\n    Returns:\n        Accuracy (float)\n    \"\"\"\n    # TODO: Implement this function (copy from last assignment)\n    y_hat = model.forward(X)\n    accuracy_count = np.count_nonzero(np.argmax(y_hat, axis=1) == np.argmax(targets, axis=1))\n    accuracy = accuracy_count/X.shape[0]\n    return accuracy",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "3b78cefb-22b6-49a2-a262-05556a3cfbe6",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2d3fb37f",
    "execution_start": 1645037816116,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "class SoftmaxTrainer(BaseTrainer):\n\n    def __init__(\n            self,\n            momentum_gamma: float,\n            use_momentum: bool,\n            *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n\n        self.momentum_gamma = momentum_gamma\n        self.use_momentum = use_momentum\n        # Init a history of previous gradients to use for implementing momentum\n        self.previous_grads = [np.zeros_like(w) for w in self.model.ws]\n        \n\n    def train_step(self, X_batch: np.ndarray, Y_batch: np.ndarray):\n        \"\"\"\n        Perform forward, backward and gradient descent step here.\n        The function is called once for every batch (see trainer.py) to perform the train step.\n        The function returns the mean loss value which is then automatically logged in our variable self.train_history.\n\n        Args:\n            X: one batch of images\n            Y: one batch of labels\n        Returns:\n            loss value (float) on batch\n        \"\"\"\n        # TODO: Implement this function (task 2c)\n\n        loss = 0\n        #weights = model.ws\n        '''\n        for i in range(len(self.model.ws)):\n            np.random.seed(0)\n            self.model.ws[i] = np.random.uniform(-1,1,(self.model.ws[i].shape))\n        '''\n        logits = self.model.forward(X_batch)\n        self.model.backward(X_batch, logits, Y_batch)\n        for i in range(len(self.model.ws)):\n            if self.use_momentum: #Task 3\n               # if i == 0:\n                #    self.model.ws[i] -= self.learning_rate*(self.model.grads[i])\n                #    self.previous_grads[i] = self.model.grads[i]\n                #else:   \n\n                deltaW = self.model.grads[i] + self.momentum_gamma*self.previous_grads[i]\n                self.model.ws[i] -= self.learning_rate*deltaW\n                self.previous_grads[i] = deltaW\n            else: #Task 2\n                self.model.ws[i] -= self.learning_rate*self.model.grads[i]\n        loss = cross_entropy_loss(Y_batch, logits) \n        return loss\n\n    def validation_step(self):\n        \"\"\"\n        Perform a validation step to evaluate the model at the current step for the validation set.\n        Also calculates the current accuracy of the model on the train set.\n        Returns:\n            loss (float): cross entropy loss over the whole dataset\n            accuracy_ (float): accuracy over the whole dataset\n        Returns:\n            loss value (float) on batch\n            accuracy_train (float): Accuracy on train dataset\n            accuracy_val (float): Accuracy on the validation dataset\n        \"\"\"\n        # NO NEED TO CHANGE THIS FUNCTION\n        logits = self.model.forward(self.X_val)\n        loss = cross_entropy_loss(self.Y_val, logits)\n\n        accuracy_train = calculate_accuracy(\n            self.X_train, self.Y_train, self.model)\n        accuracy_val = calculate_accuracy(\n            self.X_val, self.Y_val, self.model)\n        return loss, accuracy_train, accuracy_val\n",
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "4c0a451a-fefb-4812-803c-5ac8a25d7a1b",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "913b2c65",
    "execution_start": 1645037818200,
    "execution_millis": 61574,
    "deepnote_output_heights": [
     null,
     611
    ],
    "deepnote_cell_type": "code"
   },
   "source": "if __name__ == \"__main__\":\n    # hyperparameters DO NOT CHANGE IF NOT SPECIFIED IN ASSIGNMENT TEXT\n    num_epochs = 50\n    #learning_rate = .1 \n    learning_rate = .02 #For task 3\n    batch_size = 32\n    neurons_per_layer = [64, 10]\n    momentum_gamma = .9  # Task 3 hyperparameter\n    shuffle_data = True\n\n    # Settings for task 3. Keep all to false for task 2.\n    use_improved_sigmoid = True\n    use_improved_weight_init = True\n    use_momentum = True\n\n    # Load dataset\n    X_train, Y_train, X_val, Y_val = utils.load_full_mnist()\n    X_train = pre_process_images(X_train)\n    X_val = pre_process_images(X_val)\n    Y_train = one_hot_encode(Y_train, 10)\n    Y_val = one_hot_encode(Y_val, 10)\n    # Hyperparameters\n\n    model = SoftmaxModel(\n        neurons_per_layer,\n        use_improved_sigmoid,\n        use_improved_weight_init)\n    trainer = SoftmaxTrainer(\n        momentum_gamma, use_momentum,\n        model, learning_rate, batch_size, shuffle_data,\n        X_train, Y_train, X_val, Y_val,\n    )\n    train_history, val_history = trainer.train(num_epochs)\n\n    print(\"Final Train Cross Entropy Loss:\",\n          cross_entropy_loss(Y_train, model.forward(X_train)))\n    print(\"Final Validation Cross Entropy Loss:\",\n          cross_entropy_loss(Y_val, model.forward(X_val)))\n    print(\"Train accuracy:\", calculate_accuracy(X_train, Y_train, model))\n    print(\"Validation accuracy:\", calculate_accuracy(X_val, Y_val, model))\n\n    # Plot loss for first model (task 2c)\n    plt.figure(figsize=(20, 12))\n    plt.subplot(1, 2, 1)\n    plt.ylim([0., .5])\n    utils.plot_loss(train_history[\"loss\"],\n                    \"Training Loss\", npoints_to_average=10)\n    utils.plot_loss(val_history[\"loss\"], \"Validation Loss\")\n    plt.legend()\n    plt.xlabel(\"Number of Training Steps\")\n    plt.ylabel(\"Cross Entropy Loss - Average\")\n    \n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.ylim([0.80, .99])\n    utils.plot_loss(train_history[\"accuracy\"], \"Training Accuracy\")\n    utils.plot_loss(val_history[\"accuracy\"], \"Validation Accuracy\")\n    plt.xlabel(\"Number of Training Steps\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(\"task2c_train_loss.png\")",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "text": "Train shape: X: (20000, 784), Y: (20000, 1)\nValidation shape: X: (10000, 784), Y: (10000, 1)\nStandard deviation: 49.37799682313607\nMean value: 33.55274553571429\nStandard deviation: 49.06676795408154\nMean value: 33.791224489795916\nInitializing weight to shape: (785, 64)\nInitializing weight to shape: (64, 10)\n",
     "output_type": "stream"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-ab8560e2ba9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     )\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     print(\"Final Train Cross Entropy Loss:\",\n",
      "\u001b[0;32m/work/TDT4265-starter-code/assignment2/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;31m# Track validation loss / accuracy every time we progress 20% through the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnum_steps_per_val\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                     \u001b[0mtrain_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mval_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-3b0789ec98ce>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         accuracy_train = calculate_accuracy(\n\u001b[0;32m---> 71\u001b[0;31m             self.X_train, self.Y_train, self.model)\n\u001b[0m\u001b[1;32m     72\u001b[0m         accuracy_val = calculate_accuracy(\n\u001b[1;32m     73\u001b[0m             self.X_val, self.Y_val, self.model)\n",
      "\u001b[0;32m<ipython-input-2-94f6cbf785d0>\u001b[0m in \u001b[0;36mcalculate_accuracy\u001b[0;34m(X, targets, model)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# TODO: Implement this function (copy from last assignment)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0maccuracy_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_count\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-aef4b8ca189c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_improved_sigmoid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0ma_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimproved_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=fc7fa119-6ed4-4d50-aa34-54bc46270896' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "a3f0f5d5-b937-433a-94a7-d13de0165426",
  "deepnote_execution_queue": []
 }
}