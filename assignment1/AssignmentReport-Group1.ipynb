{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Assignment 1 Report",
   "metadata": {
    "cell_id": "e2ae855b-c643-42c7-b452-26753530a4f4",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": null,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n\nBefore delivery, **remember to convert this file to PDF**. You can do it in two ways:\n1. Print the webpage (ctrl+P or cmd+P)\n2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`.",
   "metadata": {
    "cell_id": "00001-68855a5c-2f4c-40d1-a2d3-d99f3bcad2ef",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# Task 1",
   "metadata": {
    "cell_id": "00002-1c52cc3d-2d70-47bb-be09-0d8b5d6e7da8",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 6,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## task 1a)\n\nWe also know that: \n$\\hat{y}^{n} = \\text{Sigmoid function} = \\sigma(z) = \\frac{1}{1+e^{-z}}$\nand $$z =  w^Tx + b $$\n\nThen we apply the chain rule to find the derivative: \n$ \\frac{\\partial C^n}{\\partial w_i} = \n\\frac{\\partial C^n}{\\partial \\hat{y}^n} \\cdot \\frac{\\partial \\hat{y}^n}{\\partial w_i} = \n\\frac{\\partial C^n}{\\partial \\hat{y}^n} \\cdot \\frac{\\partial \\hat{y}^n}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_i}$\n\nNow, we want to compute each partial derivatives and combine them:\n$\n\\frac{\\partial C^n}{\\partial \\hat{y}^n} (\n-\\left(y^{n} \\ln \\left(\\hat{y}^{n}\\right)+\\left(1-y^{n}\\right) \\ln \\left(1-\\hat{y}^{n}\\right)\\right)) = \\underline{-\\frac{y^n}{\\hat{y}^n} + \\frac{1-y^n}{1-\\hat{y}^n}} \n$\n\n$\n\\frac{\\partial \\hat{y}^n}{\\partial z}( \\frac{1}{1+e^{-z}}), \\text{ is just the derivative of the Sigmoid function, which we know is: } \n\\frac{e^{-z}}{(1+e^{-z})^2} $\n\nSince, \n$\n\\hat{y}^{2n} = \\frac{1}{(1+e^{-z})^2}\n$ \n\nWe can derive that:\n$\ne^{-z} = \\frac{1-\\hat{y}^n}{\\hat{y}^n} \n$\n\n$\n\\Rightarrow \\frac{e^{-z}}{(1+e^{-z})^2} = \\frac{1- \\hat{y}^n}{\\hat{y}^n}\\cdot\\hat{y}^{2n} = \\underline{\\hat{y}^n(1-\\hat{y}^n)}\n$\n\n$\n\\frac{\\partial z}{\\partial w_i}(w_ix + b) = \\underline{x} \n$\n\nNow we combine the three partial derivatives and simplify:\n\n$\n\\frac{\\partial C^n}{\\partial w_i} = \n\\frac{\\partial C^n}{\\partial \\hat{y}^n} \\cdot \\frac{\\partial \\hat{y}^n}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_i} = ({-\\frac{y^n}{\\hat{y}^n} + \\frac{1-y^n}{1-\\hat{y}^n}}) \\cdot \\hat{y}^n(1-\\hat{y}^n) \\cdot x = (\\frac{-y^n + \\hat{y}^n y^n + a - y^n \\hat{y}^n}{\\hat{y}^n(1-\\hat{y}^n)})\\hat{y}^n(1-\\hat{y}^n)\\cdot x \n$\n\n$\n= \\frac{\\hat{y}^n - y^n}{\\hat{y}^n(1-\\hat{y}^n)}\\hat{y}^n(1-\\hat{y}^n) \\cdot x = \\underline{\\underline{(\\hat{y}^n - y^n) x}}\n$",
   "metadata": {
    "cell_id": "00003-96178642-93c3-4225-9a7f-4fa66669890c",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 12,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## task 1b)\n\nAs from task 1a), we know that:\n$$ z =  w^Tx + b $$ and that $$\\hat{y}^n_i = \\text{ the softmax function } =  \\frac{e^{z_i}}{\\sum_k e^{z_k}}$$\n\nAlso in this task we need to break it down using the chain rule, which will give us: \n\n$ \\frac{\\partial C^n}{\\partial w_i} = \n\\frac{\\partial C^n}{\\partial \\hat{y}^n} \\cdot \\frac{\\partial \\hat{y}^n}{\\partial w_i} = \n\\frac{\\partial C^n}{\\partial \\hat{y}^n} \\cdot \\frac{\\partial \\hat{y}^n}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_i}$\n\nFirstly, we start by finding the derivative of the cost function with respect to the activation function. \n$\n\\frac{\\partial C^n}{\\partial \\hat{y}^n} = \\frac{\\partial}{\\partial \\hat{y}^n}[\\sum_k -y^n_k(ln{\\hat{y}^n_k})] =\\underline{ -\\frac{y^n_k}{\\hat{y}^n_k}}\n$\n\n\nThen we need to find the derivative of the Softmax function, here we need to consider two cases, one where the index i = j and one when i $\\neq$ j. \n\nLet us start with i = j:\n$\n\\frac{\\partial\\hat{y}^n_i}{\\partial z} = \n\\frac{e^{z_{i}}\\sum_k e^{z_{k}}-e^{z_{i}} e^{z_{i}}}{\\left(\\sum_{k} e^{z_{k}}\\right)^{2}}  = \\frac{e^{z_i}( [\\sum_k e^{z_k}] -  e^{z_i} )}{(\\sum_k e^{z_k})^2} =\\underline{\\hat{y}_i^n(1-\\hat{y}_i^n)}\n$\n\nwhen i $\\neq$ j:\n$\n\\frac{\\partial\\hat{y}^n_i}{\\partial z} =\\frac{[\\sum_k e^{z_k}]\\frac{\\partial}{\\partial z}e^{z_i} - e^{z_j} \\frac{\\partial}{\\partial z}[\\sum_k e^{z_k}]}{{\\left(\\sum_{k} e^{z_{k}}\\right)^{2}}} =  \\frac{0-e^{z_i}e^{z_j}}{{\\left(\\sum_{k} e^{z_{k}}\\right)^{2}}} = \\underline{-  \\hat{y}_i \\hat{y}_j}\n$\n\nThen we derviviate the net input with respect to the weights as in task a): \n\n$\n\\frac{\\partial z}{\\partial w_i}(w_ix + b) = \\underline{x} \n$\n\nLastly,  we apply the multivariable chain rule:\n\n$\n\\frac{\\partial C^n}{\\partial w_{kj}} = \\frac{\\partial C^n}{\\partial \\hat{y}^n_k} \\cdot \\frac{\\partial \\hat{y}^n_k}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_{k,i}} + \\frac{\\partial C^n}{\\partial \\hat{y}^n_j} \\cdot \\frac{\\partial \\hat{y}^n_j}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_{k,i}} \n$\n\n$\n= (-\\frac{y^n_k}{\\hat{y}^n_k} \\cdot \\hat{y}_i^n(1-\\hat{y}_i^n) \\cdot x_j) +( -\\frac{y^n_j}{\\hat{y}^n_j} \\cdot -  \\hat{y}_i^n \\hat{y}_j^n \\cdot x_j)\n$\n\n$\n= x_j(-y_k^n(1-\\hat{y}_k^n) + y_j^n\\hat{y}_k^n) = x_j(\\hat{y}_k^ny_k-y_k^n+y_j^n\\hat{y}_k^n) = x_j(\\hat{y}_k^n(y_k^n + y_j^n) - y_k^n)\n$\n\n$\n= \\underline{\\underline{-(y^n_k - \\hat{y}_k^n)x_j}}\n$",
   "metadata": {
    "cell_id": "00004-bd5027e3-2343-43d6-8106-bc8c83074c1a",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 18,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# Task 2",
   "metadata": {
    "cell_id": "00005-bb97fe8c-3f21-4a90-8d3d-c8e5d0db59fb",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 24,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Task 2b)\n![](task2b_binary_train_loss.png)",
   "metadata": {
    "cell_id": "00006-5512d86d-7c17-4861-aaed-212cf5515ab1",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 30,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Task 2c)\n![](task2b_binary_train_accuracy.png)",
   "metadata": {
    "cell_id": "00007-8740e7b5-9c17-4ab3-bb84-56fcbceece89",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 36,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Task 2d)\nThe early stop kicks inn after $\\underline{14}$ epochs. ",
   "metadata": {
    "cell_id": "00008-60d45d7b-b1a1-45d4-bb44-accb8406679e",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 42,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Task 2e)\nWe get less \"spikey\" behaviour when shuffeling the dataset because the network interperets every epoch as \"new\" dataset and therefore the weights will be updated with less relations to eachother then if we were to train it in the same order for each epoch. This makes the neural network more generalized and the loss converges faster. In our network the early stop kicked in at 10 epochs, as opposed to 14(without shuffeling). \n![](task2e_train_accuracy_shuffle_difference.png)",
   "metadata": {
    "cell_id": "00009-e4001ba7-6105-4b51-90ff-c8fa9949fd04",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 48,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# Task 3",
   "metadata": {
    "cell_id": "00010-5679f38e-b5e9-4b71-840f-d04f7301a571",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 54,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Task 3b)\n![](task3b_softmax_train_loss.png)",
   "metadata": {
    "cell_id": "00011-2e27070d-6899-4297-add6-58321191adfe",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 60,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Task 3c)\n![](task3b_softmax_train_accuracy.png)",
   "metadata": {
    "cell_id": "00012-f473b8f4-df30-4cad-b2bf-126e39b37a2a",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 66,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Task 3d)\n\nFrom the plots we see that model stops early, after approximately 30 epochs. This is because of the implementation of Early Stopping from task 2d, which prevents overfitting of the training dataset by looking at the validation loss. If the validation loss starts to increase, this could be a sign of overfitting. The Early Stopping algorithm interrupts the model if the validation loss has not improved for 10 consecutive validations.\n",
   "metadata": {
    "cell_id": "00013-c3aea943-bcf2-48b5-8659-789439015028",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 72,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# Task 4",
   "metadata": {
    "cell_id": "00014-45db1253-c4a8-4dbb-87b9-b50d077a357e",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 78,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Task 4a)\n$J(w) = C(w) + \\lambda R(w)$\n\nFrom task 1 we know the derivative of the cost function, which is: \n\n$C'(w) \n= -(y^n_k - \\hat{y}_k^n)x_j\n$\n\nThen we need to compute the derivative of\n\n $\\lambda R(w) = \\frac{\\lambda}{2} \\sum_{i,j}w_{i,j}^2 = \\frac{2\\lambda}{2}w_{i,j} = \\lambda w_{i,j}$\n\n Thus,\n\n $C'(w) = \\underline{\\underline{-(y^n_k - \\hat{y}_k^n)x_j + \\lambda w_{i,j}}} $\n\n\n\n\n",
   "metadata": {
    "cell_id": "00015-251364aa-32d1-42fa-8075-9ceabcc15b98",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 84,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Task 4b)\nBy looking at the equation for calculating the gradient in the backward propegation, we can see that the gradient will become greater when using a higher  $\\lambda$, like  $\\lambda=2.0$. When we update the weigths, $ w_{new} =  w - \\alpha  \\cdot \\frac{\\partial C^n(w)}{\\partial w_i} $, we see that with a greater gradient, the weight will become smaller and therefore we will experience less variations. Therefore, when using $\\lambda=2.0$, the weights for the model will become less noicy.\n\n\n![](4b.png)",
   "metadata": {
    "cell_id": "00016-47a6ac26-8a23-44bf-a91d-9b5f24b4b518",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 90,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Task 4c)\n\n![](4c.png)",
   "metadata": {
    "cell_id": "00017-67527249-3afd-4410-8a12-b7840a606291",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 96,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Task 4d)\n\nL2 regularization is a way to stop the neural network from overfitting, caused by being too dependent on certain weights (In pratice, this means stopping certan weight from beeing too big). When we include L2 regularization we find the best weights possible without them being too large. The weights are therefore less optimal for the given dataset, but more general and less prone to overfitting. Hence, we will get lower validation accuracy.\n",
   "metadata": {
    "cell_id": "00018-b087b0ed-17a6-42fb-a608-32862d6bdac8",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 102,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Task 4e)\nWe can see that the L2 norm of the weights is decreasing when $\\lambda$ is increasing.\n![](4e.png)",
   "metadata": {
    "cell_id": "00019-b85b5545-85f9-4c9b-8a04-bbc3d3511b13",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 108,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=fc7fa119-6ed4-4d50-aa34-54bc46270896' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "deepnote_notebook_id": "42e5cb4e-c195-4600-aed0-d703fd647724",
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_app_layout": "article"
 }
}