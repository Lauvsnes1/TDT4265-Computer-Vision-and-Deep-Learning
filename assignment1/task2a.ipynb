{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "be6d1928-c14d-4bad-ad78-0ed499ec0306",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e3874b3",
    "execution_start": 1643726004646,
    "execution_millis": 3085,
    "deepnote_cell_type": "code"
   },
   "source": "import numpy as np\nimport utils\nnp.random.seed(1)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0f08759f-ec6e-4a7c-a0b0-1a53a307c117",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f0e45b21",
    "execution_start": 1643726007761,
    "execution_millis": 36,
    "deepnote_cell_type": "code"
   },
   "source": "def pre_process_images(X: np.ndarray):\n    \"\"\"\n    Args:\n        X: images of shape [batch size, 784] in the range (0, 255)\n    Returns:\n        X: images of shape [batch size, 785] in the range (-1, 1)\n    \"\"\"\n    assert X.shape[1] == 784,\\\n        f\"X.shape[1]: {X.shape[1]}, should be 784\"\n    # TODO implement this function (Task 2a)\n    X = (X / (255/2)) - 1\n    bias = np.ones((X.shape[0],1), dtype='float')\n    X = np.append(X, bias, axis=1)\n    return X\n    ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "32d7d6da-18a0-45e3-b991-0156cc012459",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "30213c7",
    "execution_start": 1643726007867,
    "execution_millis": 29425,
    "deepnote_cell_type": "code"
   },
   "source": "def cross_entropy_loss(targets: np.ndarray, outputs: np.ndarray) -> float:\n    \"\"\"\n    Args:\n        targets: labels/targets of each image of shape: [batch size, 1]\n        outputs: outputs of model of shape: [batch size, 1]\n    Returns:\n        Cross entropy error (float)\n    \"\"\"\n    # TODO implement this function (Task 2a)\n    assert targets.shape == outputs.shape,\\\n        f\"Targets shape: {targets.shape}, outputs: {outputs.shape}\"\n    \n    loss = -np.average(targets*np.log(outputs) + (1-targets)*np.log(1-outputs))\n    return loss",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "e58f8cb6-3e84-4900-b6b9-603596296591",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3821c43",
    "execution_start": 1643726007914,
    "execution_millis": 43,
    "deepnote_cell_type": "code"
   },
   "source": "class BinaryModel:\n\n    def __init__(self):\n        # Define number of input nodes\n        self.I = 785\n        self.w = np.zeros((self.I, 1))\n        self.grad = None\n\n    def forward(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Args:\n            X: images of shape [batch size, 785]\n        Returns:\n            y: output of model with shape [batch size, 1]\n        \"\"\"\n        # TODO implement this function (Task 2a)\n        sigmoid = lambda z : 1/(1+np.exp(-z))\n        z = np.dot(X, self.w)\n        a = sigmoid(z)\n        return a\n\n    def backward(self, X: np.ndarray, outputs: np.ndarray, targets: np.ndarray) -> None:\n        \"\"\"\n        Computes the gradient and saves it to the variable self.grad\n        Args:\n            X: images of shape [batch size, 785]\n            outputs: outputs of model of shape: [batch size, 1]\n            targets: labels/targets of each image of shape: [batch size, 1]\n        \"\"\"\n        \n        assert targets.shape == outputs.shape,\\\n            f\"Output shape: {outputs.shape}, targets: {targets.shape}\"\n        self.grad = np.zeros_like(self.w)\n        assert self.grad.shape == self.w.shape,\\\n            f\"Grad shape: {self.grad.shape}, w: {self.w.shape}\"\n        \n        # TODO implement this function (Task 2a)\n        gradient = np.dot(X.T, (targets-outputs))\n        self.grad = -gradient/targets.shape[0]\n        \n\n    def zero_grad(self) -> None:\n        self.grad = None\n\n\ndef gradient_approximation_test(model: BinaryModel, X: np.ndarray, Y: np.ndarray):\n    \"\"\"\n        Numerical approximation for gradients. Should not be edited. \n        Details about this test is given in the appendix in the assignment.\n    \"\"\"\n    w_orig = np.random.normal(loc=0, scale=1/model.w.shape[0]**2, size=model.w.shape)\n    epsilon = 1e-3\n    for i in range(w_orig.shape[0]):\n        model.w = w_orig.copy()\n        orig = w_orig[i].copy()\n        model.w[i] = orig + epsilon\n        logits = model.forward(X)\n        cost1 = cross_entropy_loss(Y, logits)\n        model.w[i] = orig - epsilon\n        logits = model.forward(X)\n        cost2 = cross_entropy_loss(Y, logits)\n        gradient_approximation = (cost1 - cost2) / (2 * epsilon)\n        model.w[i] = orig\n        # Actual gradient\n        logits = model.forward(X)\n        model.backward(X, logits, Y)\n        difference = gradient_approximation - model.grad[i, 0]\n        assert abs(difference) <= epsilon**2,\\\n            f\"Calculated gradient is incorrect. \" \\\n            f\"Approximation: {gradient_approximation}, actual gradient: {model.grad[i,0]}\\n\" \\\n            f\"If this test fails there could be errors in your cross entropy loss function, \" \\\n            f\"forward function or backward function\"\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6145387b-48bd-46da-b7aa-d50f4d458211",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5cccf36b",
    "execution_start": 1643726007963,
    "execution_millis": 3132,
    "deepnote_output_heights": [
     null,
     306
    ],
    "deepnote_cell_type": "code"
   },
   "source": "if __name__ == \"__main__\":\n    category1, category2 = 2, 3\n    X_train, Y_train, *_ = utils.load_binary_dataset(category1, category2)\n    X_train = pre_process_images(X_train)\n    assert X_train.max() <= 1.0, f\"The images (X_train) should be normalized to the range [-1, 1]\"\n    assert X_train.min() < 0 and X_train.min() >= -1, f\"The images (X_train) should be normalized to the range [-1, 1]\"\n    assert X_train.shape[1] == 785,\\\n        f\"Expected X_train to have 785 elements per image. Shape was: {X_train.shape}\"\n\n    # Simple test for forward pass. Note that this does not cover all errors!\n    model = BinaryModel()\n    logits = model.forward(X_train)\n    np.testing.assert_almost_equal(\n        logits.mean(), .5,\n        err_msg=\"Since the weights are all 0's, the sigmoid activation should be 0.5\")\n\n    # Gradient approximation check for 100 images\n    X_train = X_train[:100]\n    Y_train = Y_train[:100]\n    for i in range(2):\n        gradient_approximation_test(model, X_train, Y_train)\n        model.w = np.random.randn(*model.w.shape)\n",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Train shape: X: (3623, 784), Y: (3623, 1)\nValidation shape: X: (426, 784), Y: (426, 1)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=fc7fa119-6ed4-4d50-aa34-54bc46270896' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "22c9e9e0-ec89-480d-92a7-3d9bccd2acd9",
  "deepnote_execution_queue": []
 }
}