{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "5e9fdd70-b961-4bc3-b6af-b386204b304b",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "613ad963",
    "execution_start": 1643990689101,
    "execution_millis": 4812,
    "deepnote_cell_type": "code"
   },
   "source": "%run task2a.ipynb\nimport numpy as np\nimport utils\n#from task2a import pre_process_images\nnp.random.seed(1)\n",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Train shape: X: (3657, 784), Y: (3657, 1)\nValidation shape: X: (415, 784), Y: (415, 1)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "d91bb05f-398b-43a3-8486-26c16f015622",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f13b7190",
    "execution_start": 1643990693934,
    "execution_millis": 103248563,
    "deepnote_cell_type": "code"
   },
   "source": "def cross_entropy_loss(targets: np.ndarray, outputs: np.ndarray):\n    \"\"\"\n    Args:\n        targets: labels/targets of each image of shape: [batch size, num_classes]\n        outputs: outputs of model of shape: [batch size, num_classes]\n    Returns:\n        Cross entropy error (float)\n    \"\"\"\n    # TODO implement this function (Task 3a)\n    assert targets.shape == outputs.shape,\\\n        f\"Targets shape: {targets.shape}, outputs: {outputs.shape}\"\n    return - np.sum(targets * np.log(outputs)) / targets.shape[0]\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "076cb0a4-4b94-40d3-9993-7adb53839c2b",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d3befe2d",
    "execution_start": 1643990693991,
    "execution_millis": 103248607,
    "deepnote_cell_type": "code"
   },
   "source": "class SoftmaxModel:\n\n    def __init__(self, l2_reg_lambda: float):\n        # Define number of input nodes\n        self.I = 785\n        # Define number of output nodes\n        self.num_outputs = 10\n        self.w = np.zeros((self.I, self.num_outputs))\n        self.grad = None\n\n        self.l2_reg_lambda = l2_reg_lambda\n\n    def forward(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Args:\n            X: images of shape [batch size, 785]\n        Returns:\n            y: output of model with shape [batch size, num_outputs]\n        \"\"\"\n        '''\n        # TODO implement this function (Task 3a)\n    \n        '''\n        softmax = lambda z : np.exp(z)/ (np.sum(np.exp(z), keepdims=True, axis=1))\n        z = np.dot(X, self.w)\n        a = softmax(z)\n        \n        return a\n\n\n    def backward(self, X: np.ndarray, outputs: np.ndarray, targets: np.ndarray) -> None:\n        \"\"\"\n        Computes the gradient and saves it to the variable self.grad\n\n        Args:\n            X: images of shape [batch size, 785]\n            outputs: outputs of model of shape: [batch size, num_outputs]\n            targets: labels/targets of each image of shape: [batch size, num_classes]\n        \"\"\"\n        # TODO implement this function (Task 3a)\n        # To implement L2 regularization task (4b) you can get the lambda value in self.l2_reg_lambda \n        # which is defined in the constructor.\n        assert targets.shape == outputs.shape,\\\n            f\"Output shape: {outputs.shape}, targets: {targets.shape}\"\n        self.grad = np.zeros_like(self.w)\n        assert self.grad.shape == self.w.shape,\\\n             f\"Grad shape: {self.grad.shape}, w: {self.w.shape}\"\n\n        gradient = np.dot(X.T, (targets-outputs))\n        self.grad = -gradient/targets.shape[0] + np.dot(self.w, self.l2_reg_lambda)\n\n    def zero_grad(self) -> None:\n        self.grad = None\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "cfbc3cf6-dfc7-450a-b7b3-a5b3e90fb4c9",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7de1d4bf",
    "execution_start": 1643990694017,
    "execution_millis": 103248554,
    "deepnote_output_heights": [
     78
    ],
    "deepnote_cell_type": "code"
   },
   "source": "def one_hot_encode(Y: np.ndarray, num_classes: int):\n    \"\"\"\n    Args:\n        Y: shape [Num examples, 1]\n        num_classes: Number of classes to use for one-hot encoding\n    Returns:\n        Y: shape [Num examples, num classes]\n    \"\"\"\n    # TODO implement this function (Task 3a) \n    res = np.zeros((len(Y),num_classes), dtype=int)\n    for i in range(len(Y)):\n        num_indx = Y[i] \n        res[i, num_indx] = 1\n    return res\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "50ff5187-3737-48c2-a0f1-239eab44ccc6",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "952fceb",
    "execution_start": 1643990694073,
    "execution_millis": 103248482,
    "deepnote_cell_type": "code"
   },
   "source": "def gradient_approximation_test(model: SoftmaxModel, X: np.ndarray, Y: np.ndarray):\n    \"\"\"\n        Numerical approximation for gradients. Should not be edited. \n        Details about this test is given in the appendix in the assignment.\n    \"\"\"\n    w_orig = np.random.normal(loc=0, scale=1/model.w.shape[0]**2, size=model.w.shape)\n\n    epsilon = 1e-3\n    for i in range(model.w.shape[0]):\n        for j in range(model.w.shape[1]):\n            model.w = w_orig.copy()\n            orig = model.w[i, j].copy()\n            model.w[i, j] = orig + epsilon\n            logits = model.forward(X)\n            cost1 = cross_entropy_loss(Y, logits)\n            model.w[i, j] = orig - epsilon\n            logits = model.forward(X)\n            cost2 = cross_entropy_loss(Y, logits)\n            gradient_approximation = (cost1 - cost2) / (2 * epsilon)\n            model.w[i, j] = orig\n            # Actual gradient\n            logits = model.forward(X)\n            model.backward(X, logits, Y)\n            difference = gradient_approximation - model.grad[i, j]\n            assert abs(difference) <= epsilon**2,\\\n                f\"Calculated gradient is incorrect. \" \\\n                f\"Approximation: {gradient_approximation}, actual gradient: {model.grad[i, j]}\\n\" \\\n                f\"If this test fails there could be errors in your cross entropy loss function, \" \\\n                f\"forward function or backward function\"",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "84f6217c-d85f-4704-bcd3-9103652340ee",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e018c0d8",
    "execution_start": 1643990694074,
    "execution_millis": 42294,
    "deepnote_output_heights": [
     78,
     405
    ],
    "deepnote_cell_type": "code"
   },
   "source": "if __name__ == \"__main__\":\n    # Simple test on one-hot encoding\n    Y = np.zeros((1, 1), dtype=int)\n    Y[0, 0] = 3\n    Y = one_hot_encode(Y, 10)\n    print(\"Y looks like this: \", Y)\n    assert Y[0, 3] == 1 and Y.sum() == 1, \\\n        f\"Expected the vector to be [0,0,0,1,0,0,0,0,0,0], but got {Y}\"\n\n    X_train, Y_train, *_ = utils.load_full_mnist()\n    X_train = pre_process_images(X_train)\n    Y_train = one_hot_encode(Y_train, 10)\n    assert X_train.shape[1] == 785,\\\n        f\"Expected X_train to have 785 elements per image. Shape was: {X_train.shape}\"\n\n    # Simple test for forward pass. Note that this does not cover all errors!\n    model = SoftmaxModel(0.0)\n    logits = model.forward(X_train)\n    np.testing.assert_almost_equal(\n        logits.mean(), 1/10,\n        err_msg=\"Since the weights are all 0's, the softmax activation should be 1/10\")\n\n    # Gradient approximation check for 100 images\n    X_train = X_train[:100]\n    Y_train = Y_train[:100]\n    for i in range(2):\n        gradient_approximation_test(model, X_train, Y_train)\n        model.w = np.random.randn(*model.w.shape)\n",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Y looks like this:  [[0 0 0 1 0 0 0 0 0 0]]\nTrain shape: X: (18000, 784), Y: (18000, 1)\nValidation shape: X: (2000, 784), Y: (2000, 1)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=fc7fa119-6ed4-4d50-aa34-54bc46270896' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "47d75119-aae6-4d37-947e-ec47498f04c9",
  "deepnote_execution_queue": []
 }
}