{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ae2e3e0a-a9f8-499b-b2b6-7238bc9812ac",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Assignment 3 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-fe966c96-8f27-462f-ab79-221d36d9a31c",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-6ebafe4c-7e81-4064-b9a2-94c381d24252",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-54b8f35b-0f02-4470-b76d-e5684797a647",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## task 1a)\n",
    "\n",
    "Since this is a convolutional kernel, there is no need to flip it. Hence, we pad it with one line zeroes in both dimentions and do the convolution by hand and get the result:\n",
    "\n",
    "![](oppg1a.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-3f23067c-148a-47bf-a93a-e0caec9f4782",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## task 1b)\n",
    "\n",
    "iii) Maxpooling reduces the sensitivity for translation since it takes a region of pixels and return the maximum intensity. By doing this, it will be more general to small translations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5036a163-9e5b-4d77-9798-554da1c0d5eb",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## task 1c)\n",
    "\n",
    "Since the stride is 1, and the kernel is of size (5x5), you need to pad the input image with 2 zeroes on each side to keep the original shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1c4fca68-b2aa-483c-ae44-d6ffac7dd3cd",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## task 1d)\n",
    "\n",
    "Since the kernel is a square the height and width is the same. We can use the formula:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W_{2} = \\frac{W_{1} − F + 2 \\cdot P}{S} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "With $W_{1}$ = 512, $W_{2}$ = 504, S = 1, P = 0 and F = x, we get F = 9. Since $W_{2} = H_{2}$, the spatial dimensions of these kernels are **$\\underline{\\underline{9\\times9}}$**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7d4da05f-fc06-455a-9673-3617c45f1bb2",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## task 1e)\n",
    "We can calculate the spatial dimension of the pooled feature maps by using the equation\n",
    "below. Since the input has a shape 504x504, the shape of the feature maps will also be a\n",
    "square, hence height = width.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& W_{2}=\\frac{\\left(W_{1}-F\\right)}{2}+1=>W_{2}=\\frac{504-2}{2}+1=252 \\\\\n",
    "&H_{2}=\\frac{\\left(H_{1}-F\\right)}{2}+1=>H_{2}=\\frac{504-2}{2}+1=252\n",
    "\\end{aligned}\n",
    "$$\n",
    "Which gives us the output image of shape **$\\underline{\\underline{252\\times252}}$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "98a658a5-e4d7-4975-9f1a-26c2a5c99667",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## task 1f)\n",
    "\n",
    "We can calculate the size of the feature maps with the same equation given in task 1b):\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W_{2} = \\frac{W_{1} − F + 2 \\cdot P}{S} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "With $W_{1}$ = 252, S = 1, P = 0 and F = 3, we get $W_{2}$ = 250. Since $W_{2} = H_{2}$, the spatial dimensions of these convolved feature maps are **$\\underline{\\underline{250\\times250}}$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "53a8dde5-d3d8-427e-a683-a0616195bd38",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## task 1g)\n",
    "![](oppg1g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-def27c7f-adef-4978-bd25-1781ce69e5a5",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2a)\n",
    "\n",
    "![](./plots/task2_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-252b3813-1664-4960-b684-eb6f952e48b6",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Task 2b)\n",
    "\n",
    "![](./uploaded_img/task2b.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-41e3f095-d04b-41cd-aaf9-9630b74fe03b",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3a)\n",
    "\n",
    "### Model 1\n",
    "\n",
    "In model 1, the alturations we made that had impact was:\n",
    "- Changing kernel size to 3x3\n",
    "- Including batch normalization for each convolutional layer.\n",
    "- Random Color Jitter \n",
    "\n",
    "The model described below includes each convolutional layer with kernel size 3x3 with a padding = 1. Every maxpool-layer has a kernel size of 2 and stride of 2. The input data was transformed with pyTorch function random Color Jitter.\n",
    "\n",
    "| Layer       | LayerType  &nbsp; &nbsp;    | &nbsp;&nbsp;Num. filters &nbsp;&nbsp;  |  &nbsp;&nbsp;&nbsp; Activation Function |\n",
    "| -------------- |:-----------------:| :----------:| :-----------:|\n",
    "|    1        |  Conv2D          |        32         |   ReLU     |\n",
    "|    1        |  BatchNorm2D         |                |   ReLU     |\n",
    "|    1        | MaxPool2D          |      -           |   -     |\n",
    "|    2        |  Conv2D          |        64         |   ReLU     |\n",
    "|    2        |  BatchNorm2D         |                |   ReLU     |\n",
    "|    2        |  MaxPool2D          |      -           |   -     |\n",
    "|     3       |    Conv2D        |        128         |  ReLU      |\n",
    "|    3        |  BatchNorm2D         |                |   ReLU     |\n",
    "|    3        |  MaxPool2D          |      -           |   -     |\n",
    "|            |    Flatten        |         -        |    -    |\n",
    "|    4        |  Fully-Connected          |      64           |   ReLU     |\n",
    "|    5        |  Fully-Connected          |      10           |   Softmax     |\n",
    "\n",
    "### Model 2\n",
    "\n",
    "In model 2, we tried altering some other hyperparameters and techniques. The ones that made a positive impact was:\n",
    "- ASGD optimizer\n",
    "- ReLU6 activation function.\n",
    "- Dropout\n",
    "\n",
    "The model described below includes each convolutional layer with kernel size 3x3 with a padding = 1. Every maxpool-layer has a kernel size of 2 and stride of 2. We also included a dropout after the last convolutional layer with a probability of the neuron to be dropped set to 25%. Changing from regular Relu activation to ReLU6 increaced the accuracy a bit, but the most significant improvement was due to changing the optimzer to ASGD - averaged stochastic gradient descent. We also implemented the dropout technique which is to \"drop\" certain neurons, making the network more general and less dependent on certain neurons.   \n",
    "\n",
    "| Layer       | LayerType  &nbsp; &nbsp;    | &nbsp;&nbsp;Num. filters &nbsp;&nbsp;  |  &nbsp;&nbsp;&nbsp; Activation Function |\n",
    "| -------------- |:-----------------:| :----------:| :-----------:|\n",
    "|    1        |  Conv2D          |        32         |   ReLU6     |\n",
    "|    1        | MaxPool2D          |      -           |   -     |\n",
    "|    2        |  Conv2D          |        64         |   ReLU6     |\n",
    "|    2        |  MaxPool2D          |      -           |   -     |\n",
    "|     3       |    Conv2D        |        128         |  ReLU6      |\n",
    "|    3        |  MaxPool2D          |      -           |   -     |\n",
    "|            |    Flatten        |         -        |    -    |\n",
    "|    4        |  Fully-Connected          |      64           |   ReLU6     |\n",
    "|    5        |  Fully-Connected          |      10           |   Softmax     |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "\n",
    "| Model        | Train loss  &nbsp; &nbsp;    | &nbsp;&nbsp;Train accuracy &nbsp;&nbsp;  |  &nbsp;&nbsp;&nbsp; Validation accuracy | &nbsp;&nbsp;&nbsp; Test accuracy |\n",
    "| -------------- |:-----------------:| :----------:| :-----------:| :-----------:|\n",
    "| Model 1       |         0.534           |  81.2%           |     72.3%         |      75.2%        |\n",
    "| Model 2       |        0.398            |    86.6%         |        77.1%      |      75.6%        |\n",
    "\n",
    "\n",
    "\n",
    "### Model 2 (the best)\n",
    "![](./plots/task3_model2_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "\n",
    "**SGD &rarr;** ASGD was the best improvement for our model. The ASGD often accelirates the stochastic gradient descent, also in our model. Therefore it is more likely to reach a local/global minimum faster. \n",
    "\n",
    "**Data transformations**In the beginning we tried a lot of different transformations to the data. Most of these game worse accuracy and seem to make the network worse as well. This could be because the dataset is not large enough, only 1000 photos. Therefore it makes sense that altering the too much with the photos will make it difficoult for the network to \"memorize\" the data, leading to worse results. However, some of the transformations made smome small improvements, the random color jitter, random horizontal flip and random crop. Since these transformations doesnt alter too much with the images, it has a positive effect to generalization of the network, even with a relatively small dataset. \n",
    "\n",
    "**Batch norm**: Batch normalization is done after the congolutional layer and normalizes the feature maps. This makes it easier for the network to extract the right features to learn from and prohibits certain features from dominating the learning.  \n",
    "\n",
    "**ReLU6**: The Relu6 restricts the normal relu activationfunction on the positive side, bu defining it as f(x) = min(max(0,x), 6). This prevents the gradients to potentially going to infity, which may damage the learning.  \n",
    "\n",
    "**Kernel size** Firstly we tried changing the kernel size to 7x7, which led to worse results for our model(approx. 2%). On the other side changing the kernel to 3x3 led to an improvement in accuracy. We believe this in mainy because a 3x3 kernel is more fixed on details, and less prone to sorroundings fore each pixel, especially when the image has such a low resolution (32x32). With the same logic, a 7x7 kernel is too big for such a small resolution and will not detetect the correct change in features. \n",
    "\n",
    "**Momentum** When implementing momentum to SGD, we experienced much worse accuracy and we suspect this was due to the gradient taking too large steps at a time. Momentum needs to be carefully implemented with the right learning rate, otherwise it would overshoot and perform worse. However, we did not find the right balance here and momentum caused worse accuracy.\n",
    "\n",
    "**Dropout**. We implemented dropout to make the model less dependent on certain neurons and more generilzed. This gave us better accuracy, which indicates a more generlized model. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "\n",
    "![](./plots/task3d_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3e)\n",
    "\n",
    "![](./plots/task3_model_e_plot.png)\n",
    "\n",
    "\n",
    "![](./uploaded_img/task3e.png)\n",
    "\n",
    "\n",
    "\n",
    "**Final test accuracy** =  83.4%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3f)\n",
    "\n",
    "We see that the accuracy graphs fro train and test/validation follows eachother quite closely. This indicates that there is neither over or underfitting. The reason for this is perhaps because we used dropout after both the convolutional layers and the fully connected layers. There is a small gap between train/test and validation graph, but this is as expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-305955dd-e54f-4c3d-b433-34d75216231a",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-5b6b1b57-3495-4f78-8cd1-c83bc552f123",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Task 4a)\n",
    "\n",
    "![](./plots/task4a_plot.png)\n",
    "\n",
    "![](./uploaded_img/task4a.png)\n",
    "\n",
    "**Final test accuracy** = 90.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-a6bb8910-1789-457a-bab9-76de81b73f27",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Task 4b)\n",
    "\n",
    "![](./plots/task_4b.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First filter**: This filter corresponds to vertical edge detection. That is why we clearly see the vertical stripes on the zebra. \n",
    "\n",
    "**Second filter**: This filter corresponds to horizontal edge detection, which is why we clearly see that horizontal strpes on the zebra.\n",
    "\n",
    "**Third filter**: This filters seems to extract color. We can see that the sky is extracted, while other features are blurred.\n",
    "\n",
    "**Fourth filter**: This filter seems to detect diagonal edges. We can see the diagonal stripes on the zebra, while the other stripes are faded. \n",
    "\n",
    "**Fifth filter**: This filters seems to extract color, invers of the third filter. We can see that the grass is extracted, while other features are blurred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-1f65537d-4148-4f35-90e0-c1439a580359",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Task 4c)\n",
    "\n",
    "![](./plots/task_4c.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at these 10 filters we can see that the activations are mostly focused around the zebra. Some of them seems to extract features from the head, like filter 2, 3 and 7, while others extract from the body or the legs, for example 1 (legs) and 4 (body). We can also see that the other part of the pictures are not focues on by these filter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=fc7fa119-6ed4-4d50-aa34-54bc46270896' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "e71432ff-cb80-4431-aed7-b549af5becd0",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}