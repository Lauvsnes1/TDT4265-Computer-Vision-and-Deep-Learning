{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "Intersect over union is an evaluation metric which is used to measure accuracy for an object detector on a dataset. It calculates the difference between ground truth and the predicted bounding box by:\n",
    "- calculating the area of intersection\n",
    "- Calculate the total union area\n",
    "- then $ IoU = \\frac{\\text{area of intersect}}{ \\text{area of union}}$\n",
    "\n",
    "![](./imported_img/IoU_1a.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "Recall is another metric of precision used in object detection. This indicates the ability to find the ground truths, but doesn´t take in to account the objects that are overlooked. \n",
    "\n",
    "$ Recall = \\frac{TP}{TP + FN}$\n",
    "\n",
    " Precision however, are only how many relevant predictions the network can make. This means how many true positives there are in all the detections.\n",
    "\n",
    "$ Precision = \\frac{TP}{TP + FP}$\n",
    "\n",
    "\n",
    "\n",
    " True positives are objects detected which corresponds with the ground truth(correct prediction on correct object). False positives are when the network labels an object when it is not present(desired prediction on wrong object).   \n",
    "\n",
    "\n",
    "\n",
    "## task 1c)\n",
    "\n",
    "Firstly, we make a sorted table with precision and recall:\n",
    "\n",
    "Class 1: \n",
    "| no. | precision | recall|\n",
    "|:-----:|:----:|:-----:|\n",
    "|  1   |  1.0   | 0.05  |\n",
    "|   2  |  1.0   | 0.1  |\n",
    "|   3  | 1.0    | 0.4  |\n",
    "|   4  |   0.50  |  0.7 |\n",
    "|   5  |   0.20  | 1.0  |\n",
    "\n",
    "Then we use the 11 point interpolation to find the Average presicion:\n",
    "\n",
    "$ AP_1 = \\frac{1}{11}(1 \\times 5 + 0.5 \\times 3 + 0.2 \\times 3) = \\frac{71}{110} = 64.54 \\% $\n",
    "\n",
    "![](./imported_img/Class1.jpeg)\n",
    "\n",
    "Class 2: \n",
    "| no. | precision | recall|\n",
    "|:-----:|:----:|:-----:|\n",
    "|  1   |  1.0   | 0.3  |\n",
    "|   2  |  0.8   | 0.4  |\n",
    "|   3  |  0.6   | 0.5  |\n",
    "|   4  |   0.50 |  0.7 |\n",
    "|   5  |   0.20 | 1.0  |\n",
    "\n",
    "Then we use the 11 point interpolation to find the Average presicion:\n",
    "\n",
    "$ AP_2 = \\frac{1}{11}(1 \\times 4 + 0.8 \\times 1 + 0.6 \\times 1 + 0.5 \\times 2 + 0.2 \\times 3) = \\frac{7}{11} = 63.63 \\% $\n",
    "\n",
    "![](./imported_img/Class2.jpeg)\n",
    "\n",
    "Thus, the mean average percision(mAP) is $ \\frac{1}{n} \\Sigma AP$, where n is number of classes.\n",
    "\n",
    "$ mAP = \\frac{\\frac{71}{110} + \\frac{7}{11}}{2} =  \\underline{\\underline{64.091}} \\%$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2f)\n",
    "\n",
    "![](./task2/precision_recall_curve.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "\n",
    "This filtering operation is called Non-maximum suppression (NMS)\n",
    "\n",
    "### Task 3b)\n",
    "**Is the following true or false: Predictions from the deeper layers in SSD are responsible to\n",
    "detect small objects.**\n",
    "\n",
    "This is false because the deeper layers have lower resolution and are therefore responsible to detect large objects. High resolution feature maps are responsible to detect small objects.\n",
    "\n",
    "### Task 3c)\n",
    "The reason we use anchors with different bounding box aspect ratios at same spatial location is that different objects come in different sizes and shapes. Say you were to detect people and houses with an SSD. These objects often come with very different sizes and shapes, especially if they appear in the same image, therefore we use several bounding boxes with different aspect ratios during the training. \n",
    "\n",
    "### Task 3d)\n",
    "The key difference between SSD and the YOLO model is that YOLO can be extremely quick(155fps) and is therefore preferable in cases where swiftness is key. SSD is slower, but can deliver higher accuracy. \n",
    "\n",
    "### Task 3e)\n",
    "The amount of anchor boxes depends on how many grid cell the image is splitted into. Since we are in the first layer of the SSD, the spatial resolution of this feature map is (38 x 38). Since we use 6 anchors for each cell in the grid, we get:\n",
    "$ (38 \\times 38) \\times 6 = \\underline{\\underline{8664}} $\n",
    "\n",
    "### Task 3f)\n",
    "Using the same logic as task 3e), the total number og anchors in the network will be:\n",
    "$ ((38 \\times 38) + (19 \\times 19) + (10 \\times 10)  + (5 \\times 5)  + (3 \\times 3)  + (1 \\times 1)) \\times 6 = \\underline{\\underline{11640}} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "\n",
    "![](./imported_img/task4b_total_loss.png)\n",
    "\n",
    "**Mean average precision:**  mPA@0.5 = 0.746\n",
    "\n",
    "## Task 4c)\n",
    "FILL IN ANSWER. \n",
    "**Mean average precision** mAP@0.5 = 0.902\n",
    "\n",
    "- LeakyRelu with weightinit - Kaiming_uniform/He\n",
    "- BatchNormalization\n",
    "- Dropout\n",
    "- Sample Crop\n",
    "- Change ST.dev and Mean to optimal\n",
    "- Adam optim lr= 10-3\n",
    "- one extra conv layer in each feature extractor\n",
    "- Reduced the sizes of the achor boxes, as the model was worst on small digits\n",
    "- Increased output channels. two last ones, 64 and 64 to 256 and 512, respectively\n",
    "\n",
    "\n",
    "\n",
    "## Task 4d)\n",
    "| (32, 32) | (95, 32) | (158, 32) | (221,32)|(284, 32) |\n",
    "|:-----:|:----:|:-----:|:---:|:---:|\n",
    "|   (32,95)  |  (95,95 )   | (158,95)  | (221,95) | (284,95) |\n",
    "|   (32,158)  |  (95,158 )  | (158,158) |(221, 158) |(284, 158)|\n",
    "|   (32,221)  | (95, 221) | (158,221) |(221,221) |(284,221)|\n",
    "|   (32,284)  |   (95,284 )  | (158,284) |(221,284) |(284,284)|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Box 1: Square of side min_size: $162²$\n",
    "\n",
    "Box 2: Square of side $\\sqrt{min size \\cdot next min size} = \\sqrt{162*213}² = 185.7 = [13.6, 13.6]$\n",
    "\n",
    "Aspect ratio = 2:\n",
    "\n",
    "Box 3: [162 * $\\sqrt{2}, 162 / \\sqrt{2}] = [229.1, 114.6]$\n",
    "\n",
    "Box 4: [162 / $\\sqrt{2}, 162 * \\sqrt{2}] = [114.6, 229.1]$\n",
    "\n",
    "Aspect ratio = 3:\n",
    "\n",
    "Box 5: [162 * $\\sqrt{3}, 162 / \\sqrt{3}] = [280.6, 93.5]$\n",
    "\n",
    "Box 6: [162 / $\\sqrt{3}, 162 * \\sqrt{3}] = [93.5, 280.6]$\n",
    "\n",
    "\n",
    "## Task 4e)\n",
    "Missed:\n",
    "- Image 1: 5 and 1\n",
    "- Image 3: 1\n",
    "- Image 4: 4\n",
    "\n",
    "Since we altered the min_sizes array to include smaller anchor boxes, our model were also able to detect small digits in the images, but includes some false positives on small details.\n",
    "\n",
    "Example image: \n",
    "![]()\n",
    "\n",
    "## Task 4f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
